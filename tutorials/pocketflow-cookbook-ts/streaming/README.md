# PocketFlow LLM Streaming Tutorial

!!! NOT COMPLETE

Real-time streaming of Large Language Model responses with user interruption capabilities using PocketFlow and TypeScript. Perfect for building responsive chat applications and interactive AI systems.

## Features

- **Real-Time Response Streaming**: See LLM-generated content appear character-by-character as it's being produced
- **User Interruption**: Press ENTER at any time to gracefully stop the streaming process
- **Production-Ready**: Full TypeScript implementation with proper error handling
- **Flexible Integration**: Easy to integrate into web applications, CLIs, or other interactive systems
- **Memory Efficient**: Streams responses without loading entire content into memory

## Quick Setup & Run

1. **Install dependencies:**
    ```bash
    npm install
    ```

2. **Set your OpenAI API key:**
    ```bash
    export OPENAI_API_KEY="your-api-key-here"
    ```
    
    Or create a `.env` file:
    ```bash
    echo "OPENAI_API_KEY=your-api-key-here" > .env
    ```

3. **Test streaming (fake/demo mode):**
    ```bash
    npm run demo
    ```

4. **Run with real OpenAI streaming:**
    ```bash
    npm start
    ```

5. **Try different prompts:**
    ```bash
    npm start "Write a story about space exploration"
    npm start "Explain quantum computing in simple terms"
    ```

## How It Works

```mermaid
flowchart LR
    A[User Input] --> B[StreamNode]
    B --> C[LLM Streaming<br/>+ Interrupt Listener]
    C --> D[Real-time Display]
    C -->|ENTER pressed| E[Stop Gracefully]
    D --> F[Complete]
    E --> F
```

**Simple concept:** Stream LLM responses in real-time while allowing users to interrupt anytime by pressing ENTER.

### Key Components

#### ðŸŒŠ **StreamNode**
Manages the streaming lifecycle with three main phases:
- **`prep()`**: Set up interrupt listener and prepare the prompt for streaming
- **`exec()`**: Stream LLM response chunks in real-time while monitoring for interruptions
- **`post()`**: Clean up resources and display completion status

#### âš¡ **Real-Time Processing**
- Uses OpenAI's streaming API to get response chunks immediately
- Displays content character-by-character for smooth user experience
- Maintains responsiveness even during long responses

#### ðŸ›‘ **Interrupt System**
- Non-blocking input listener using readline interface
- Graceful cancellation that doesn't corrupt partial responses
- Clean resource cleanup when interrupted

## Example Output

```
ðŸŒŠ PocketFlow LLM Streaming Demo
===============================

Enter your prompt (or press ENTER for default): Write a short poem about TypeScript

ðŸ¤– Streaming response... (Press ENTER to interrupt)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ In the realm of code where types hold sway, â”‚
â”‚ TypeScript guides us along the way.         â”‚
â”‚ With interfaces clean and functions true,   â”‚
â”‚ It catches our errors before they're due.   â”‚
â”‚                                             â”‚
â”‚ No more the chaos of JavaScript's dance,    â”‚
â”‚ Type safety gives our code a chance...      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ… Stream completed successfully!
ðŸ“Š Stats: 156 characters streamed in 3.2 seconds
```

## API Usage

You can also use the streaming functionality programmatically:

```typescript
import { StreamNode } from './main';

// Basic streaming
const streamNode = new StreamNode();
const result = await streamNode.streamResponse("Your prompt here");

// With custom interrupt handling
const customStreamNode = new StreamNode({
  onInterrupt: () => console.log("Stream was interrupted!"),
  onChunk: (chunk) => console.log("Received:", chunk),
  onComplete: (fullText) => console.log("Final result:", fullText)
});
```

## Customization

### Streaming Behavior
Modify streaming parameters in `main.ts`:
```typescript
const response = await openai.chat.completions.create({
  model: 'gpt-4',
  messages: [{ role: 'user', content: prompt }],
  stream: true,
  temperature: 0.7,        // Adjust creativity
  max_tokens: 1000,        // Limit response length
  presence_penalty: 0.1    // Reduce repetition
});
```

### Display Formatting
Customize the visual presentation:
```typescript
// Change streaming display style
process.stdout.write(`\x1b[32m${chunk}\x1b[0m`); // Green text
process.stdout.write(`\x1b[1m${chunk}\x1b[0m`);  // Bold text

// Add custom borders or formatting
console.log(`â•­â”€ ${timestamp} â”€â•®`);
console.log(`â”‚ ${chunk} â”‚`);
console.log(`â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯`);
```

### Interrupt Handling
Add custom interrupt logic:
```typescript
private setupInterruptListener(): void {
  this.rl.on('line', () => {
    this.interrupted = true;
    console.log('\nðŸ›‘ Stream interrupted by user');
    // Custom cleanup logic here
    this.cleanup();
  });
}
```

## Advanced Usage

### Web Integration
Stream to web clients using Server-Sent Events:
```typescript
// Express.js example
app.get('/stream', async (req, res) => {
  res.setHeader('Content-Type', 'text/event-stream');
  const streamNode = new StreamNode({
    onChunk: (chunk) => res.write(`data: ${JSON.stringify({chunk})}\n\n`)
  });
  await streamNode.streamResponse(req.query.prompt);
  res.end();
});
```

### Batch Processing
Stream multiple responses in sequence:
```typescript
const prompts = ["Question 1", "Question 2", "Question 3"];
for (const prompt of prompts) {
  console.log(`\n--- Streaming: ${prompt} ---`);
  await streamNode.streamResponse(prompt);
}
```

## Files

- [`main.ts`](./main.ts): StreamNode implementation and flow orchestration
- [`utils.ts`](./utils.ts): OpenAI streaming utilities and demo functions
- [`package.json`](./package.json): Node.js package configuration
- [`tsconfig.json`](./tsconfig.json): TypeScript compilation settings

## Technical Implementation

This streaming tutorial demonstrates several advanced patterns:

- **Asynchronous Stream Processing**: Handle real-time data streams with proper backpressure
- **Concurrent Input Monitoring**: Listen for user input while processing streams
- **Resource Management**: Clean up streams and listeners to prevent memory leaks
- **Error Recovery**: Graceful handling of network interruptions and API errors
- **User Experience**: Smooth, responsive interfaces that feel immediate and interactive

The implementation showcases how PocketFlow can handle complex, real-time scenarios while maintaining clean, readable code structure.

## Use Cases

This streaming pattern is perfect for:

- **Interactive Chatbots**: Immediate response feedback in conversations
- **Code Generation Tools**: See generated code appear in real-time
- **Content Creation**: Watch articles, stories, or documentation being written
- **Educational Assistants**: Stream explanations and tutorials progressively
- **Creative Writing**: Real-time story generation and collaborative writing

Ready to build responsive, streaming AI applications with PocketFlow! ðŸš€
